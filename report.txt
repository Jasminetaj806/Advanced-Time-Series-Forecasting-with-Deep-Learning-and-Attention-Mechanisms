The Attention-based Seq2Seq LSTM was selected to address long-range temporal dependencies that traditional SARIMA and feedforward MLP models fail to capture. The encoder compresses historical information, while the decoder dynamically attends to relevant time steps for each forecast horizon. Hyperparameter tuning was conducted over hidden units and batch sizes, with 64 units and batch size 32 yielding the lowest validation MAE. Compared to SARIMA, the Attention LSTM reduced MAE by X percent and demonstrated superior adaptability to non-linear temporal patterns.
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

preds = model.predict([X_test, decoder_input_test])
preds = preds.reshape(-1)
y_true = y_test.reshape(-1)

mae = mean_absolute_error(y_true, preds)
rmse = mean_squared_error(y_true, preds, squared=False)
mape = np.mean(np.abs((y_true - preds) / y_true)) * 100

print(f"MAE  : {mae:.4f}")
print(f"RMSE : {rmse:.4f}")
print(f"MAPE : {mape:.2f}%")